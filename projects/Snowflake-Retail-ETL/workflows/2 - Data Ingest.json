{
  "name": "2 - Data Ingest",
  "uuid": "fe1cff46-6b6c-41c8-a7c6-3aeb669dd406",
  "category": "-",
  "nodes": [
    {
      "id": "3",
      "name": "Read CSV",
      "description": "It reads in CSV files and creates a DataFrame from it",
      "details": "\u003ch2\u003eRead CSV Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node reads CSV files and creates a DataFrame from them. It can read either a single file or a directory containing multiple files. The user can configure the below fields to parse the file.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eOutput storage level\u003c/b\u003e from the drop down. The options in the dropdown can be one of the following:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY\u003c/b\u003e          Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they are needed. This is the default level.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK\u003c/b\u003e       Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that do not fit on disk, and read them from there when they are needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_SER\u003c/b\u003e        Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK_SER\u003c/b\u003e    Similar to MEMORY_ONLY_SER, but spill partitions that do not fit in memory to disk instead of recomputing them on the fly each time they\u0027re needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDISK_ONLY\u003c/b\u003e              Store the RDD partitions only on disk.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_2, MEMORY_AND_DISK_2 others \u003c/b\u003e . Same as the levels above, but replicate each partition on two cluster nodes.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eOFF_HEAP\u003c/b\u003e               Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.\u003c/li\u003e\n\u003c/ul\u003e\nThe user needs to provide a data file \u003cb\u003ePath\u003c/b\u003e to read the data from. This is a required field.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eSeparator\u003c/b\u003e used in the data file to parse it. The default separator is \u003cb\u003e( , )\u003c/b\u003e comma.\u003cbr\u003e\n\u003cbr\u003e\nIn the \u003cb\u003eHeader\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e if the data file has a header.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eDrop special character in column name\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e If you want to remove the special characters from column names.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eMode\u003c/b\u003e field, one can choose from the below options in the dropdown:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003ePERMISSIVE\u003c/b\u003e When the parser meets a corrupt field in a record, it sets the value of the field to NULL and continues to the next record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDROPMALFORMED\u003c/b\u003e ignores the whole corrupted record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eFAILFAST\u003c/b\u003e throws an exception when it meets corrupted records.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eEnforce Schema\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e The specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e The schema will be validated against all headers in CSV files when the header option is set to \u003cb\u003efalse\u003c/b\u003e.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eWhether to add input file as a column in DataFrame\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e There will be a new column added to the DataFrame at the end, which can be seen in the schema columns. One can enter the name of this column.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e This functionality is disabled, and the DataFrame consists of only the columns read from the data file.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eENCODING\u003c/b\u003e field, one can specify the encoding type to be used for reading the files. By default, it is set as \u003cb\u003eUTF-8\u003c/b\u003e.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eQUOTE\u003c/b\u003e field sets a single character used for escaping quoted values where the separator can be part of the value. The default value for this is \u003cb\u003e( \" )\u003c/b\u003e, a double quote.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eESCAPE\u003c/b\u003e field sets a single character used for escaping quotes inside an already quoted value. The default value for this is \u003cb\u003e( \\ )\u003c/b\u003e, a backslash.\t\u003cbr\u003e\n\u003cbr\u003e\nAfter the above options are set, one can click on \u003cb\u003eRefresh Schema\u003c/b\u003e to see the final columns.\u003cbr\u003e\nUsers can still add or delete columns using \u003cb\u003e+\u003c/b\u003e button next to the refresh schema and \u003cb\u003e-\u003c/b\u003e button next to the column names.\u003cbr\u003e",
      "examples": "",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "293px",
      "y": "208px",
      "hint": "Refresh the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "path",
          "value": "/home/sparkflows/fire-data/ETL_Sample_Data/ETL_Payment.csv",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to add Input File Name as a column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Currency_Type\",\"Transaction_ID\",\"Price\",\"Tax\",\"Discount_Perc\",\"Payment_Terms\",\"Transaction_Date\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "4",
      "name": "Read CSV",
      "description": "It reads in CSV files and creates a DataFrame from it",
      "details": "\u003ch2\u003eRead CSV Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node reads CSV files and creates a DataFrame from them. It can read either a single file or a directory containing multiple files. The user can configure the below fields to parse the file.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eOutput storage level\u003c/b\u003e from the drop down. The options in the dropdown can be one of the following:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY\u003c/b\u003e          Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they are needed. This is the default level.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK\u003c/b\u003e       Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that do not fit on disk, and read them from there when they are needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_SER\u003c/b\u003e        Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK_SER\u003c/b\u003e    Similar to MEMORY_ONLY_SER, but spill partitions that do not fit in memory to disk instead of recomputing them on the fly each time they\u0027re needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDISK_ONLY\u003c/b\u003e              Store the RDD partitions only on disk.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_2, MEMORY_AND_DISK_2 others \u003c/b\u003e . Same as the levels above, but replicate each partition on two cluster nodes.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eOFF_HEAP\u003c/b\u003e               Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.\u003c/li\u003e\n\u003c/ul\u003e\nThe user needs to provide a data file \u003cb\u003ePath\u003c/b\u003e to read the data from. This is a required field.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eSeparator\u003c/b\u003e used in the data file to parse it. The default separator is \u003cb\u003e( , )\u003c/b\u003e comma.\u003cbr\u003e\n\u003cbr\u003e\nIn the \u003cb\u003eHeader\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e if the data file has a header.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eDrop special character in column name\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e If you want to remove the special characters from column names.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eMode\u003c/b\u003e field, one can choose from the below options in the dropdown:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003ePERMISSIVE\u003c/b\u003e When the parser meets a corrupt field in a record, it sets the value of the field to NULL and continues to the next record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDROPMALFORMED\u003c/b\u003e ignores the whole corrupted record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eFAILFAST\u003c/b\u003e throws an exception when it meets corrupted records.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eEnforce Schema\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e The specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e The schema will be validated against all headers in CSV files when the header option is set to \u003cb\u003efalse\u003c/b\u003e.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eWhether to add input file as a column in DataFrame\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e There will be a new column added to the DataFrame at the end, which can be seen in the schema columns. One can enter the name of this column.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e This functionality is disabled, and the DataFrame consists of only the columns read from the data file.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eENCODING\u003c/b\u003e field, one can specify the encoding type to be used for reading the files. By default, it is set as \u003cb\u003eUTF-8\u003c/b\u003e.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eQUOTE\u003c/b\u003e field sets a single character used for escaping quoted values where the separator can be part of the value. The default value for this is \u003cb\u003e( \" )\u003c/b\u003e, a double quote.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eESCAPE\u003c/b\u003e field sets a single character used for escaping quotes inside an already quoted value. The default value for this is \u003cb\u003e( \\ )\u003c/b\u003e, a backslash.\t\u003cbr\u003e\n\u003cbr\u003e\nAfter the above options are set, one can click on \u003cb\u003eRefresh Schema\u003c/b\u003e to see the final columns.\u003cbr\u003e\nUsers can still add or delete columns using \u003cb\u003e+\u003c/b\u003e button next to the refresh schema and \u003cb\u003e-\u003c/b\u003e button next to the column names.\u003cbr\u003e",
      "examples": "",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "449px",
      "y": "277px",
      "hint": "Refresh the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "path",
          "value": "/home/sparkflows/fire-data/ETL_Sample_Data/ETL_Products.csv",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to add Input File Name as a column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Product_ID\",\"Product_Group_Code\",\"Product_Group_Desc\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"INTEGER\",\"INTEGER\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "5",
      "name": "Join On Common Column",
      "description": "This node joins the incoming dataframes using one common column between them.",
      "details": "\u003ch2\u003eJoin On Common Column Node Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node joins the incoming dataframes using one common column between the two dataframes. \u003cbr\u003e\n\u003cbr\u003e\nSelect the Common Join Column to be used in the Join.\u003cbr\u003e\n\u003cbr\u003e\nJoining modes supported by this node is as follows:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e inner : The joined table will have records that have matching values in both tables.\u003c/li\u003e\n\u003cli\u003e outer : The joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.\u003c/li\u003e\n\u003cli\u003e left_outer  : Even if there are no matches in the right table it returns all the rows from the left table.\u003c/li\u003e\n\u003cli\u003e right_outer : Even if there are no matches in the left table it returns all the rows from the right table.\u003c/li\u003e\n\u003cli\u003e leftsemi : This gives only those rows in the left table that have a matching row in the right table.\u003c/li\u003e\n\u003cli\u003e leftanti : This join returns rows in the left table that have no matching rows in the right table.\u003c/li\u003e\n\u003c/ul\u003e",
      "examples": "\u003ch2\u003eJoin On Common Column Example\u003c/h2\u003e\n\u003cbr\u003e\n\u003ch4\u003e Incoming Datasets\u003c/h4\u003e\n\u003cbr\u003e\n1st Incoming Dataframe table1 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10         \u003cbr\u003e\nE02       |    JOHN        |    20      \u003cbr\u003e\nE03       |    MARTIN      |    30  \u003cbr\u003e\nE04       |    TONY        |    40  \u003cbr\u003e\n\u003cbr\u003e\n2nd Incoming Dataframe table2 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nDEPT_NO    |      DEPT_NAME   |    LOC       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\n10         |      HR          |    IND  \u003cbr\u003e\n20         |      SALES       |    AUS  \u003cbr\u003e\n30         |      MARKETING   |    UK         \u003cbr\u003e\n50         |      RESEARCH    |    NZ      \u003cbr\u003e\n\u003cbr\u003e\nThe common join column is DEPT_NO\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `inner` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |      DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |      HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |      SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |      MARKETING  |    UK\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n          |                |              |   RESEARCH   |    NZ\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `left_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `right_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n          \u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\n          |                |    50        |   RESEARCH   |    NZ\u003cbr\u003e",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingColumn",
      "x": "296px",
      "y": "74px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinCol",
          "value": "Customer_ID",
          "widget": "variable_common",
          "title": "Common Join Column",
          "description": "column on which to join",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinType",
          "value": "inner",
          "widget": "array",
          "title": "JoinType",
          "description": "type of join",
          "optionsArray": [
            "inner",
            "outer",
            "leftouter",
            "rightouter",
            "leftsemi",
            "leftanti"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Customer_ID\",\"Customer_Type\",\"Customer_Name\",\"Country\",\"State\",\"City\",\"Sales_Org\",\"Quantity\",\"Order_ID\",\"Order_Date\",\"Product_ID\",\"Sales_Channel\",\"Transaction_ID\",\"Order_Status\"]",
          "widget": "schema_col_names",
          "title": "Output Column Names",
          "description": "Name of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"DOUBLE\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"DOUBLE\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Output Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Output Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "6",
      "name": "Join On Common Column",
      "description": "This node joins the incoming dataframes using one common column between them.",
      "details": "\u003ch2\u003eJoin On Common Column Node Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node joins the incoming dataframes using one common column between the two dataframes. \u003cbr\u003e\n\u003cbr\u003e\nSelect the Common Join Column to be used in the Join.\u003cbr\u003e\n\u003cbr\u003e\nJoining modes supported by this node is as follows:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e inner : The joined table will have records that have matching values in both tables.\u003c/li\u003e\n\u003cli\u003e outer : The joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.\u003c/li\u003e\n\u003cli\u003e left_outer  : Even if there are no matches in the right table it returns all the rows from the left table.\u003c/li\u003e\n\u003cli\u003e right_outer : Even if there are no matches in the left table it returns all the rows from the right table.\u003c/li\u003e\n\u003cli\u003e leftsemi : This gives only those rows in the left table that have a matching row in the right table.\u003c/li\u003e\n\u003cli\u003e leftanti : This join returns rows in the left table that have no matching rows in the right table.\u003c/li\u003e\n\u003c/ul\u003e",
      "examples": "\u003ch2\u003eJoin On Common Column Example\u003c/h2\u003e\n\u003cbr\u003e\n\u003ch4\u003e Incoming Datasets\u003c/h4\u003e\n\u003cbr\u003e\n1st Incoming Dataframe table1 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10         \u003cbr\u003e\nE02       |    JOHN        |    20      \u003cbr\u003e\nE03       |    MARTIN      |    30  \u003cbr\u003e\nE04       |    TONY        |    40  \u003cbr\u003e\n\u003cbr\u003e\n2nd Incoming Dataframe table2 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nDEPT_NO    |      DEPT_NAME   |    LOC       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\n10         |      HR          |    IND  \u003cbr\u003e\n20         |      SALES       |    AUS  \u003cbr\u003e\n30         |      MARKETING   |    UK         \u003cbr\u003e\n50         |      RESEARCH    |    NZ      \u003cbr\u003e\n\u003cbr\u003e\nThe common join column is DEPT_NO\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `inner` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |      DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |      HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |      SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |      MARKETING  |    UK\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n          |                |              |   RESEARCH   |    NZ\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `left_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `right_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n          \u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\n          |                |    50        |   RESEARCH   |    NZ\u003cbr\u003e",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingColumn",
      "x": "443px",
      "y": "136px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinCol",
          "value": "Transaction_ID",
          "widget": "variable_common",
          "title": "Common Join Column",
          "description": "column on which to join",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinType",
          "value": "inner",
          "widget": "array",
          "title": "JoinType",
          "description": "type of join",
          "optionsArray": [
            "inner",
            "outer",
            "leftouter",
            "rightouter",
            "leftsemi",
            "leftanti"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Transaction_ID\",\"Customer_ID\",\"Customer_Type\",\"Customer_Name\",\"Country\",\"State\",\"City\",\"Sales_Org\",\"Quantity\",\"Order_ID\",\"Order_Date\",\"Product_ID\",\"Sales_Channel\",\"Order_Status\",\"Currency_Type\",\"Price\",\"Tax\",\"Discount_Perc\",\"Payment_Terms\",\"Transaction_Date\"]",
          "widget": "schema_col_names",
          "title": "Output Column Names",
          "description": "Name of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"STRING\",\"STRING\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Output Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Output Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "7",
      "name": "Join On Common Column",
      "description": "This node joins the incoming dataframes using one common column between them.",
      "details": "\u003ch2\u003eJoin On Common Column Node Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node joins the incoming dataframes using one common column between the two dataframes. \u003cbr\u003e\n\u003cbr\u003e\nSelect the Common Join Column to be used in the Join.\u003cbr\u003e\n\u003cbr\u003e\nJoining modes supported by this node is as follows:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e inner : The joined table will have records that have matching values in both tables.\u003c/li\u003e\n\u003cli\u003e outer : The joined table contains either all the records from both the tables or fills in NULL values for missing matches on either side.\u003c/li\u003e\n\u003cli\u003e left_outer  : Even if there are no matches in the right table it returns all the rows from the left table.\u003c/li\u003e\n\u003cli\u003e right_outer : Even if there are no matches in the left table it returns all the rows from the right table.\u003c/li\u003e\n\u003cli\u003e leftsemi : This gives only those rows in the left table that have a matching row in the right table.\u003c/li\u003e\n\u003cli\u003e leftanti : This join returns rows in the left table that have no matching rows in the right table.\u003c/li\u003e\n\u003c/ul\u003e",
      "examples": "\u003ch2\u003eJoin On Common Column Example\u003c/h2\u003e\n\u003cbr\u003e\n\u003ch4\u003e Incoming Datasets\u003c/h4\u003e\n\u003cbr\u003e\n1st Incoming Dataframe table1 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10         \u003cbr\u003e\nE02       |    JOHN        |    20      \u003cbr\u003e\nE03       |    MARTIN      |    30  \u003cbr\u003e\nE04       |    TONY        |    40  \u003cbr\u003e\n\u003cbr\u003e\n2nd Incoming Dataframe table2 has the following rows:\u003cbr\u003e\n\u003cbr\u003e\nDEPT_NO    |      DEPT_NAME   |    LOC       \u003cbr\u003e\n-------------------------------------------\u003cbr\u003e\n10         |      HR          |    IND  \u003cbr\u003e\n20         |      SALES       |    AUS  \u003cbr\u003e\n30         |      MARKETING   |    UK         \u003cbr\u003e\n50         |      RESEARCH    |    NZ      \u003cbr\u003e\n\u003cbr\u003e\nThe common join column is DEPT_NO\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `inner` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |      DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |      HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |      SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |      MARKETING  |    UK\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n          |                |              |   RESEARCH   |    NZ\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition is `left_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\nE04       |    TONY        |    40        |              |    \u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003e When the Joining condition `right_outer` we have\u003c/h4\u003e\n\u003ch4\u003e Final Output\u003c/h4\u003e\n          \u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    DEPT_NO   |   DEPT_NAME  |    LOC           \u003cbr\u003e\n--------------------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    10        |   HR         |    IND\u003cbr\u003e\nE02       |    JOHN        |    20        |   SALES      |    AUS\u003cbr\u003e\nE03       |    MARTIN      |    30        |   MARKETING  |    UK\u003cbr\u003e\n          |                |    50        |   RESEARCH   |    NZ\u003cbr\u003e",
      "type": "join",
      "nodeClass": "fire.nodes.etl.NodeJoinUsingColumn",
      "x": "572px",
      "y": "195px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinCol",
          "value": "Product_ID",
          "widget": "variable_common",
          "title": "Common Join Column",
          "description": "column on which to join",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "joinType",
          "value": "inner",
          "widget": "array",
          "title": "JoinType",
          "description": "type of join",
          "optionsArray": [
            "inner",
            "outer",
            "leftouter",
            "rightouter",
            "leftsemi",
            "leftanti"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Product_ID\",\"Transaction_ID\",\"Customer_ID\",\"Customer_Type\",\"Customer_Name\",\"Country\",\"State\",\"City\",\"Sales_Org\",\"Quantity\",\"Order_ID\",\"Order_Date\",\"Sales_Channel\",\"Order_Status\",\"Currency_Type\",\"Price\",\"Tax\",\"Discount_Perc\",\"Payment_Terms\",\"Transaction_Date\",\"Product_Group_Code\",\"Product_Group_Desc\"]",
          "widget": "schema_col_names",
          "title": "Output Column Names",
          "description": "Name of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"INTEGER\",\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"STRING\",\"DOUBLE\",\"DOUBLE\",\"DOUBLE\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Output Column Types",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Output Column Formats",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "8",
      "name": "Count",
      "description": "This node counts the number of records in the incoming Dataframe and puts the count into result page.",
      "details": "This node counts the number of records in the incoming Dataframe and puts the count into result page.\u003cbr\u003e\n\u003cbr\u003e\nVariable Name to display count value and option to display it in the output can be specified in the node.\u003cbr\u003e",
      "examples": "Incoming Dataframe has following rows:\u003cbr\u003e\n\u003cbr\u003e\nEMP_CD    |    EMP_NAME    |    LOCATION    |    DEPT         |    SALARY\u003cbr\u003e\n-----------------------------------------------------------------------------\u003cbr\u003e\nE01       |    DAVID       |    NEW YORK    |    HR           |    10000\u003cbr\u003e\nE02       |    JOHN        |    NEW JERSEY  |    SALES        |    11000\u003cbr\u003e\nE03       |    MARTIN      |    NEW YORK    |    MARKETING    |    12000\u003cbr\u003e\nE04       |    TONY        |    NEW JERSEY  |    MARKETING    |    13000\u003cbr\u003e\nE05       |    ROSS        |    NEW YORK    |    FRONT DESK   |    10000\u003cbr\u003e\nE06       |    LISA        |    NEW JERSEY  |    FRONT DESK   |    11000\u003cbr\u003e\nE07       |    PAUL        |    NEW YORK    |    MAINTENANCE  |    12000\u003cbr\u003e\nE08       |    MARK        |    NEW JERSEY  |    MAINTENANCE  |    13000\u003cbr\u003e\n\u003cbr\u003e\nif Count node is configured to display count value against [EMP_COUNT] variable then it would be displayed in the output as below:\u003cbr\u003e\n\u003cbr\u003e\nEMP_COUNT      :    8\u003cbr\u003e",
      "type": "transform",
      "nodeClass": "fire.nodes.etl.NodeCount",
      "x": "767px",
      "y": "77px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "variable",
          "value": "count",
          "widget": "textfield",
          "title": "Variable Name",
          "description": "Name of the Variable in which the count is stored",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "printCount",
          "value": "true",
          "widget": "array",
          "title": "PrintCount",
          "description": "Print the count into result page.",
          "datatypes": [
            "boolean"
          ],
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "scala"
    },
    {
      "id": "9",
      "name": "Print N Rows",
      "description": "Prints the specified number of records in the DataFrame. It is useful for seeing intermediate output",
      "details": "\u003ch2\u003ePrint N Rows Node Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node is used to print the first N rows from the incoming dataframe.\u003cbr\u003e\n\u003cbr\u003e\nThe Number of rows that needs to be printed can be configured in the node.\u003cbr\u003e\n\u003cbr\u003e\n\u003ch4\u003eInput Parameters\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e OUTPUT STORAGE LEVEL : Keep this as DEFAULT.\u003c/li\u003e\n\u003cli\u003e TITLE : Enter a short description for the type of information being displayed.\u003c/li\u003e\n\u003cli\u003e NUM ROWS TO PRINT : Set an integer value(N) which controls the number of rows to be displayed(Default N\u003d10).\u003c/li\u003e\n\u003cli\u003e DISPLAY DATA TYPE : Shows the output dataframe column datatypes by default.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eOutput\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003e This node can be used to view, analyze and validate the output of the Dataframe.\u003c/li\u003e\n\u003c/ul\u003e",
      "examples": "",
      "type": "transform",
      "nodeClass": "fire.nodes.util.NodePrintFirstNRows",
      "x": "766px",
      "y": "196px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "title",
          "value": "Row Values",
          "widget": "textfield",
          "title": "Title",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "n",
          "value": "10",
          "widget": "textfield",
          "title": "Num Rows to Print",
          "description": "number of rows to be printed",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "displayDataType",
          "value": "true",
          "widget": "array",
          "title": "Display Data Type",
          "description": "If true display rows DataType",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "10",
      "name": "Read JDBC",
      "description": "This node reads data from Relational Databases using JDBC and creates a DataFrame from it",
      "details": "This node reads data from Relational Databases using JDBC and creates a DataFrame from it.\u003cbr\u003e",
      "examples": "",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetJDBCUsingConnection",
      "x": "135px",
      "y": "29px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "connection",
          "value": "1",
          "widget": "object_array",
          "title": "Connection",
          "description": "The JDBC connection to connect",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "dbtable",
          "value": "userdb.customers",
          "widget": "textarea_small",
          "title": "DB Table",
          "description": "The JDBC table that should be read. Note that anything that is valid in a FROM clause of a SQL query can be used. For example, instead of a full table, you could also use a subquery in parentheses",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "performance",
          "value": "",
          "widget": "tab",
          "title": "Performance",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "partitionColumn",
          "value": "",
          "widget": "textfield",
          "title": "Partition Column",
          "description": "PartitionColumn must be a numeric, date, or timestamp column from the table",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "lowerBound",
          "value": "",
          "widget": "textfield",
          "title": "Lower Bound",
          "description": " LowerBound and UpperBound are just used to decide the partition stride, not for filtering the rows in the table. All rows in the table will be partitioned and returned. This option applies only to reading",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "upperBound",
          "value": "",
          "widget": "textfield",
          "title": "Upper Bound",
          "description": " LowerBound and UpperBound are just used to decide the partition stride, not for filtering the rows in the table. All rows in the table will be partitioned and returned. This option applies only to reading",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "numPartitions",
          "value": "",
          "widget": "textfield",
          "title": "Num Partitions",
          "description": "The maximum number of partitions that can be used for parallelism in table reading",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "fetchsize",
          "value": "",
          "widget": "textfield",
          "title": "Fetch Size",
          "description": "The JDBC fetch size, which determines how many rows to fetch per round trip",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "schema",
          "value": "",
          "widget": "tab",
          "title": "Schema",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Customer_Type\",\"Customer_ID\",\"Customer_Name\",\"Country\",\"State\",\"City\"]",
          "widget": "schema_col_names",
          "title": "Column Names of the Table",
          "description": "Output Columns Names of the Table",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"DOUBLE\",\"STRING\",\"STRING\",\"STRING\",\"STRING\"]",
          "widget": "schema_col_types",
          "title": "Column Types of the Table",
          "description": "Output Column Types of the Table",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats",
          "description": "Output Column Formats",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "scala"
    },
    {
      "id": "11",
      "name": "Read CSV",
      "description": "It reads in CSV files and creates a DataFrame from it",
      "details": "\u003ch2\u003eRead CSV Details\u003c/h2\u003e\n\u003cbr\u003e\nThis node reads CSV files and creates a DataFrame from them. It can read either a single file or a directory containing multiple files. The user can configure the below fields to parse the file.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eOutput storage level\u003c/b\u003e from the drop down. The options in the dropdown can be one of the following:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY\u003c/b\u003e          Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they are needed. This is the default level.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK\u003c/b\u003e       Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that do not fit on disk, and read them from there when they are needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_SER\u003c/b\u003e        Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_AND_DISK_SER\u003c/b\u003e    Similar to MEMORY_ONLY_SER, but spill partitions that do not fit in memory to disk instead of recomputing them on the fly each time they\u0027re needed.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDISK_ONLY\u003c/b\u003e              Store the RDD partitions only on disk.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eMEMORY_ONLY_2, MEMORY_AND_DISK_2 others \u003c/b\u003e . Same as the levels above, but replicate each partition on two cluster nodes.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eOFF_HEAP\u003c/b\u003e               Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to be enabled.\u003c/li\u003e\n\u003c/ul\u003e\nThe user needs to provide a data file \u003cb\u003ePath\u003c/b\u003e to read the data from. This is a required field.\u003cbr\u003e\n\u003cbr\u003e\nThe user can choose the \u003cb\u003eSeparator\u003c/b\u003e used in the data file to parse it. The default separator is \u003cb\u003e( , )\u003c/b\u003e comma.\u003cbr\u003e\n\u003cbr\u003e\nIn the \u003cb\u003eHeader\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e if the data file has a header.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eDrop special character in column name\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e If you want to remove the special characters from column names.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e Otherwise.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eMode\u003c/b\u003e field, one can choose from the below options in the dropdown:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003ePERMISSIVE\u003c/b\u003e When the parser meets a corrupt field in a record, it sets the value of the field to NULL and continues to the next record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eDROPMALFORMED\u003c/b\u003e ignores the whole corrupted record.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003eFAILFAST\u003c/b\u003e throws an exception when it meets corrupted records.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eEnforce Schema\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e The specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e The schema will be validated against all headers in CSV files when the header option is set to \u003cb\u003efalse\u003c/b\u003e.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eWhether to add input file as a column in DataFrame\u003c/b\u003e field, one can choose:\u003cbr\u003e\n\u003cul\u003e\n\u003cli\u003e \u003cb\u003etrue\u003c/b\u003e There will be a new column added to the DataFrame at the end, which can be seen in the schema columns. One can enter the name of this column.\u003c/li\u003e\n\u003cli\u003e \u003cb\u003efalse\u003c/b\u003e This functionality is disabled, and the DataFrame consists of only the columns read from the data file.\u003c/li\u003e\n\u003c/ul\u003e\nIn the \u003cb\u003eENCODING\u003c/b\u003e field, one can specify the encoding type to be used for reading the files. By default, it is set as \u003cb\u003eUTF-8\u003c/b\u003e.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eQUOTE\u003c/b\u003e field sets a single character used for escaping quoted values where the separator can be part of the value. The default value for this is \u003cb\u003e( \" )\u003c/b\u003e, a double quote.\u003cbr\u003e\n\u003cbr\u003e\nThe \u003cb\u003eESCAPE\u003c/b\u003e field sets a single character used for escaping quotes inside an already quoted value. The default value for this is \u003cb\u003e( \\ )\u003c/b\u003e, a backslash.\t\u003cbr\u003e\n\u003cbr\u003e\nAfter the above options are set, one can click on \u003cb\u003eRefresh Schema\u003c/b\u003e to see the final columns.\u003cbr\u003e\nUsers can still add or delete columns using \u003cb\u003e+\u003c/b\u003e button next to the refresh schema and \u003cb\u003e-\u003c/b\u003e button next to the column names.\u003cbr\u003e",
      "examples": "",
      "type": "dataset",
      "nodeClass": "fire.nodes.dataset.NodeDatasetCSV",
      "x": "142px",
      "y": "168px",
      "hint": "Refresh the schema when a new file is selected or the file content has changed",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "path",
          "value": "/home/sparkflows/fire-data/ETL_Sample_Data/ETL_Order.csv",
          "widget": "textfield",
          "title": "Path",
          "description": "Path of the file/directory",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "separator",
          "value": ",",
          "widget": "textfield",
          "title": "Separator",
          "description": "CSV Separator",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Whether the file has a header row",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "dropSpecialCharacterInColumnName",
          "value": "false",
          "widget": "array",
          "title": "Drop Special Character In ColumnName",
          "description": "Whether to drop the Special Characters and Spaces in Column Name.",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "mode",
          "value": "PERMISSIVE",
          "widget": "array",
          "title": "Mode",
          "description": "Mode for dealing with corrupt records during parsing.",
          "optionsArray": [
            "PERMISSIVE",
            "DROPMALFORMED",
            "FAILFAST"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "enforceSchema",
          "value": "false",
          "widget": "array",
          "title": "Enforce Schema",
          "description": "If it is set to true, the specified or inferred schema will be forcibly applied to datasource files, and headers in CSV files will be ignored. If the option is set to false, the schema will be validated against all headers in CSV files in the case when the header option is set to true.",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "addInputFileName",
          "value": "false",
          "widget": "array",
          "title": "Whether to add Input File Name as a column in the Dataframe",
          "description": "Add the new field:input_file_name",
          "optionsArray": [
            "false",
            "true"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColNames",
          "value": "[\"Sales_Org\",\"Quantity\",\"Order_ID\",\"Order_Date\",\"Product_ID\",\"Sales_Channel\",\"Transaction_ID\",\"Order_Status\",\"Customer_ID\"]",
          "widget": "schema_col_names",
          "title": "Column Names for the CSV",
          "description": "New Output Columns of the SQL",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColTypes",
          "value": "[\"STRING\",\"INTEGER\",\"STRING\",\"STRING\",\"INTEGER\",\"STRING\",\"DOUBLE\",\"STRING\",\"DOUBLE\"]",
          "widget": "schema_col_types",
          "title": "Column Types for the CSV",
          "description": "Data Type of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "outputColFormats",
          "value": "[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]",
          "widget": "schema_col_formats",
          "title": "Column Formats for the CSV",
          "description": "Format of the Output Columns",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    },
    {
      "id": "12",
      "name": "Save CSV",
      "description": "Saves the DataFrame into the specified location in CSV Format",
      "details": "This node saves incoming Dataframe into the specified location in CSV format.\u003cbr\u003e",
      "examples": "",
      "type": "transform",
      "nodeClass": "fire.nodes.save.NodeSaveCSV",
      "x": "769px",
      "y": "325px",
      "fields": [
        {
          "name": "storageLevel",
          "value": "DEFAULT",
          "widget": "array",
          "title": "Output Storage Level",
          "description": "Storage Level of the Output Datasets of this Node",
          "optionsArray": [
            "DEFAULT",
            "NONE",
            "DISK_ONLY",
            "DISK_ONLY_2",
            "MEMORY_ONLY",
            "MEMORY_ONLY_2",
            "MEMORY_ONLY_SER",
            "MEMORY_ONLY_SER_2",
            "MEMORY_AND_DISK",
            "MEMORY_AND_DISK_2",
            "MEMORY_AND_DISK_SER",
            "MEMORY_AND_DISK_SER_2",
            "OFF_HEAP"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "path",
          "value": "/home/sparkflows/fire-data/ETL_Sample_Data/Combined_Order_Data",
          "widget": "textfield",
          "title": "Path",
          "description": "Path where to save the CSV files",
          "required": true,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "saveMode",
          "value": "Overwrite",
          "widget": "array",
          "title": "Save Mode",
          "description": "Whether to Append, Overwrite or Error if the path Exists",
          "optionsArray": [
            "Append",
            "Overwrite",
            "ErrorIfExists",
            "Ignore"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "header",
          "value": "true",
          "widget": "array",
          "title": "Header",
          "description": "Should a Header Row be saved with each File?",
          "optionsArray": [
            "true",
            "false"
          ],
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "encoding",
          "value": "UTF-8",
          "widget": "textfield",
          "title": "Encoding",
          "description": "Decodes the CSV files by the given encoding type",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "quote",
          "value": "\"",
          "widget": "textfield",
          "title": "Quote",
          "description": "Sets a single character used for escaping quoted values where the separator can be part of the value",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "escape",
          "value": "\\",
          "widget": "textfield",
          "title": "Escape",
          "description": "Sets a single character used for escaping quotes inside an already quoted value.",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "advanced",
          "value": "",
          "widget": "tab",
          "title": "Advanced",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        },
        {
          "name": "partitionColNames",
          "value": "[]",
          "widget": "variables",
          "title": "Partition Column Names",
          "description": "Partition Column Names",
          "required": false,
          "display": true,
          "editable": true,
          "disableRefresh": false,
          "expandable": false
        }
      ],
      "engine": "all"
    }
  ],
  "edges": [
    {
      "source": "5",
      "target": "6",
      "id": 1
    },
    {
      "source": "3",
      "target": "6",
      "id": 2
    },
    {
      "source": "6",
      "target": "7",
      "id": 3
    },
    {
      "source": "4",
      "target": "7",
      "id": 4
    },
    {
      "source": "7",
      "target": "8",
      "id": 5
    },
    {
      "source": "7",
      "target": "9",
      "id": 6
    },
    {
      "source": "10",
      "target": "5",
      "id": 7
    },
    {
      "source": "11",
      "target": "5",
      "id": 8
    },
    {
      "source": "7",
      "target": "12",
      "id": 9
    }
  ],
  "dataSetDetails": [],
  "engine": "scala"
}